{
  "spec_version": "1.0",
  "metadata": {
    "id": "ollama-nvidia",
    "name": "Ollama - NVIDIA",
    "description": "Get up and running with Llama 3, Mistral, Gemma, and other large language models.",
    "tagline": "LLMs inference server with OpenAI compatible API",
    "version": "0.17.4",
    "author": "BigBearTechWorld",
    "developer": "ollama",
    "category": "BigBearCasaOS",
    "license": "",
    "homepage": "https://hub.docker.com/r/ollama/ollama",
    "source": "big-bear-universal",
    "created": "2025-10-27T02:42:07Z",
    "updated": "2026-02-27T04:50:06.902Z"
  },
  "visual": {
    "icon": "https://cdn.jsdelivr.net/gh/selfhst/icons/png/ollama.png",
    "thumbnail": "",
    "screenshots": [],
    "logo": "https://cdn.jsdelivr.net/gh/selfhst/icons/png/ollama.png"
  },
  "resources": {
    "youtube": "",
    "documentation": "",
    "repository": "",
    "issues": "",
    "support": "https://community.bigbeartechworld.com/"
  },
  "technical": {
    "architectures": [
      "amd64",
      "arm64"
    ],
    "platform": "linux",
    "main_service": "big-bear-ollama-nvidia",
    "default_port": "11434",
    "main_image": "ollama/ollama",
    "compose_file": "docker-compose.yml"
  },
  "deployment": {
    "environment_variables": [],
    "volumes": [
      {
        "container": "/root/.ollama",
        "description": "Container Path: /root/.ollama"
      }
    ],
    "ports": [
      {
        "container": "11434",
        "host": "11434",
        "protocol": "tcp",
        "description": "Container Port: 11434"
      }
    ]
  },
  "ui": {
    "scheme": "http",
    "path": "",
    "tips": {}
  },
  "compatibility": {
    "casaos": {
      "supported": true,
      "port_map": "11434",
      "volume_mappings": {
        "ollama-nvidia_.ollama": "/DATA/AppData/$AppID/.ollama"
      },
      "port": "11434"
    },
    "portainer": {
      "supported": true,
      "template_type": 2,
      "categories": [
        "BigBearCasaOS",
        "selfhosted"
      ],
      "administrator_only": false,
      "port": "11434"
    },
    "runtipi": {
      "supported": true,
      "tipi_version": 1,
      "supported_architectures": [
        "amd64",
        "arm64"
      ],
      "volume_mappings": {
        "ollama-nvidia_.ollama": ".ollama"
      },
      "port": "11436"
    },
    "dockge": {
      "supported": true,
      "file_based": true,
      "port": "11434"
    },
    "cosmos": {
      "supported": true,
      "servapp": true,
      "routes_required": true,
      "port": "11434"
    },
    "umbrel": {
      "supported": true,
      "manifest_version": 1,
      "volume_mappings": {
        "ollama-nvidia_.ollama": ".ollama"
      },
      "port": "11436"
    }
  },
  "tags": [
    "selfhosted",
    "docker",
    "bigbear",
    "bigbearcasaos",
    "container"
  ]
}
